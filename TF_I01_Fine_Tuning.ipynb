{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwHb1l93G+Uu9/+az+zEZr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakshitgosain/Transformers-for-NLP/blob/main/TF_I01_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text processing Review\n",
        "\n",
        "Steps:\n",
        "1. Tokenizing- String split, split words,+ punctuations, characters, subwords\n",
        "2. Map tokens to integers . E.g I like cats= [420, 650, 103]\n",
        "3. Padding/truncation (to process batches of different length)\n",
        "\n",
        "\n",
        "Tokenization\n",
        "* strtok function - splits the sting by a deliminator\n",
        "* It's but now more than just splitting by a Deliminator. We need more steps . Eg - Lemmatization, stemming, stopwords removal\n",
        "\n",
        "Character-level Tokenization\n",
        "- BOB likes cats -> B,O,B,\"\",l,i,k,e,s...\n",
        "- Such are used for use cases like : language translation, name generation.\n",
        "\n",
        "Subword Token\n",
        "It is customary to use subword tokeniztion in Transformers.\n",
        "* sometimes we split words into multiple tokens.\n",
        "* consider- run, running, etc.\n",
        "* with word-level tokenization, these would be treated independently.\n",
        "* Not ideal , because we know there's some relationship.\n",
        "* Better with 2 tokens- run +suffix\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wwb2W7Vs6gif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Subword tokenization and trnasformers\n",
        "\n",
        "* Although we ofter use subword tokeniztion with transformers, they are separate concepts\n",
        "* If you are using RNNs, we can also use subword tokenization with RNN.\n",
        "\n",
        "Subword tokenizatio- How do we choose subword boundaries\n",
        "\n",
        "We can apply algos to tokenize texts\n",
        "\n"
      ],
      "metadata": {
        "id": "48-5cv7l8oVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###NOTE: Different models use different tokeization techniques"
      ],
      "metadata": {
        "id": "haO4EkL3-p0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mapping tokens to integers\n",
        "\n",
        "We represent a mapping with dict\n",
        "\n",
        "We also use the reverse-mapping"
      ],
      "metadata": {
        "id": "eqpYAjOk_DJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding\n",
        "\n",
        "Every doc has a different length which is not good for processing in batchs..so we need a constant size.\n",
        "\n",
        "We cannot assume that it's always 0\n",
        "\n",
        "** We do not want to pad everything to be as long as the longest document in the whole dataset( There can be outliers\n",
        "* EG- Most if the docs have 10k words but one doc had 100k words/tokens\n",
        "* We should pad dynamically, relative to the current batch\n",
        "* It's ok for different batches to have different lengths.\n",
        "* Transformers cannot be recurrent, so they cannot handle sequences of different lengths\n"
      ],
      "metadata": {
        "id": "r2XLnx8S_dZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Truncation\n",
        "\n",
        "Opposite of padding.\n",
        "* We want our docs to be shorter.\n",
        "There is a limit of characters for transformers"
      ],
      "metadata": {
        "id": "aYTxc-2c_kis"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jcMlDTROA0Vl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}